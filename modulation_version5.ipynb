{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:37:24.508361600Z",
     "start_time": "2023-12-10T00:37:24.490356900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part0.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part1.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part2.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part3.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part4.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part5.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part6.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part7.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part8.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part9.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part10.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part11.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part12.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part13.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part14.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part15.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part16.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part17.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part18.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part19.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part20.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part21.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part22.h5\n",
      "F:\\sourceCode\\AMC-2\\ExtractDataset\\part23.h5\n",
      "shape of X_train： (599040, 1024, 2)\n",
      "shape of Y_train： (599040, 24)\n",
      "shape of X_test： (74880, 1024, 2)\n",
      "shape of Y_test： (74880, 24)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##########################################\n",
    "# Due to hardware limitations, part of the data is extracted from the complete data set and divided into 24 parts.\n",
    "# Each part corresponds to a modulation, with 1200*26=31200 pieces of data\n",
    "# Therefore, the current data set size is 748800*1024*2\n",
    "##########################################\n",
    "def split_data(X_data, Y_data, train_frac=0.8, valid_frac=0.1):\n",
    "    n_examples = X_data.shape[0]\n",
    "    n_train = int(n_examples * train_frac)\n",
    "    n_valid = int(n_examples * valid_frac)\n",
    "\n",
    "    indices = np.arange(n_examples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_idx = indices[:n_train]\n",
    "    valid_idx = indices[n_train:n_train + n_valid]\n",
    "    test_idx = indices[n_train + n_valid:]\n",
    "\n",
    "    X_train, Y_train = X_data[train_idx], Y_data[train_idx]\n",
    "    X_valid, Y_valid = X_data[valid_idx], Y_data[valid_idx]\n",
    "    X_test, Y_test = X_data[test_idx], Y_data[test_idx]\n",
    "\n",
    "    return X_train, Y_train, X_valid, Y_valid, X_test, Y_test\n",
    "\n",
    "# Create an empty list to stack data\n",
    "X_train, Y_train, X_valid, Y_valid, X_test, Y_test = [], [], [], [], [], []\n",
    "\n",
    "for i in range(24):\n",
    "\n",
    "    filename = f'F:\\sourceCode\\AMC-2\\ExtractDataset\\part{i}.h5'\n",
    "    print(filename)\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        X_data = f['X'][:]\n",
    "        Y_data = f['Y'][:]\n",
    "\n",
    "    X_tr, Y_tr, X_val, Y_val, X_te, Y_te = split_data(X_data, Y_data)\n",
    "    if i == 0:\n",
    "        X_train, Y_train = X_tr, Y_tr\n",
    "        X_valid, Y_valid = X_val, Y_val\n",
    "        X_test, Y_test = X_te, Y_te\n",
    "    else:\n",
    "        X_train = np.vstack((X_train, X_tr))\n",
    "        Y_train = np.vstack((Y_train, Y_tr))\n",
    "        X_valid = np.vstack((X_valid, X_val))\n",
    "        Y_valid = np.vstack((Y_valid, Y_val))\n",
    "        X_test = np.vstack((X_test, X_te))\n",
    "        Y_test = np.vstack((Y_test, Y_te))\n",
    "\n",
    "print('shape of X_train：',X_train.shape)\n",
    "print('shape of Y_train：',Y_train.shape)\n",
    "print('shape of X_test：',X_test.shape)\n",
    "print('shape of Y_test：',Y_test.shape)\n",
    "\n",
    "# for i in range(0,17): #17个数据集文件\n",
    "#     ########打开文件#######\n",
    "#     filename = f'F:\\ISEP_Learning_Document\\Semester3\\End-of-track Project\\dataset\\data_raw\\Dataset\\part{i}.h5'\n",
    "#     print(filename)\n",
    "#     f = h5py.File(filename,'r')\n",
    "#     ########读取数据#######\n",
    "#     X_data = f['X'][:]\n",
    "#     Y_data = f['Y'][:]\n",
    "#     Z_data = f['Z'][:]\n",
    "#     f.close()\n",
    "#     #########分割训练集和测试集#########\n",
    "#     #每读取到一个数据文件就直接分割为训练集和测试集，防止爆内存\n",
    "#     n_examples = X_data.shape[0]\n",
    "#     n_train = int(n_examples * 0.7)   #70%训练样本\n",
    "#     train_idx = np.random.choice(range(0,n_examples), size=n_train, replace=False)#随机选取训练样本下标\n",
    "#     test_idx = list(set(range(0,n_examples))-set(train_idx))        #测试样本下标\n",
    "#     if i == 0:\n",
    "#         X_train = X_data[train_idx]\n",
    "#         Y_train = Y_data[train_idx]\n",
    "#         Z_train = Z_data[train_idx]\n",
    "#         X_test = X_data[test_idx]\n",
    "#         Y_test = Y_data[test_idx]\n",
    "#         Z_test = Z_data[test_idx]\n",
    "#     else:\n",
    "#         X_train = np.vstack((X_train, X_data[train_idx]))\n",
    "#         Y_train = np.vstack((Y_train, Y_data[train_idx]))\n",
    "#         Z_train = np.vstack((Z_train, Z_data[train_idx]))\n",
    "#         X_test = np.vstack((X_test, X_data[test_idx]))\n",
    "#         Y_test = np.vstack((Y_test, Y_data[test_idx]))\n",
    "#         Z_test = np.vstack((Z_test, Z_data[test_idx]))\n",
    "# print('训练集X维度：',X_train.shape)\n",
    "# print('训练集Y维度：',Y_train.shape)\n",
    "# print('训练集Z维度：',Z_train.shape)\n",
    "# print('测试集X维度：',X_test.shape)\n",
    "# print('测试集Y维度：',Y_test.shape)\n",
    "# print('测试集Z维度：',Z_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:37:52.570054200Z",
     "start_time": "2023-12-10T00:37:24.653049800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modulated signal distribution in the training set：\n",
      "{'OOK': 24960, '4ASK': 24960, '8ASK': 24960, 'BPSK': 24960, 'QPSK': 24960, '8PSK': 24960, '16PSK': 24960, '32PSK': 24960, '16APSK': 24960, '32APSK': 24960, '64APSK': 24960, '128APSK': 24960, '16QAM': 24960, '32QAM': 24960, '64QAM': 24960, '128QAM': 24960, '256QAM': 24960, 'AM-SSB-WC': 24960, 'AM-SSB-SC': 24960, 'AM-DSB-WC': 24960, 'AM-DSB-SC': 24960, 'FM': 24960, 'GMSK': 24960, 'OQPSK': 24960}\n",
      "\n",
      "Modulated signal distribution in test set：\n",
      "{'OOK': 3120, '4ASK': 3120, '8ASK': 3120, 'BPSK': 3120, 'QPSK': 3120, '8PSK': 3120, '16PSK': 3120, '32PSK': 3120, '16APSK': 3120, '32APSK': 3120, '64APSK': 3120, '128APSK': 3120, '16QAM': 3120, '32QAM': 3120, '64QAM': 3120, '128QAM': 3120, '256QAM': 3120, 'AM-SSB-WC': 3120, 'AM-SSB-SC': 3120, 'AM-DSB-WC': 3120, 'AM-DSB-SC': 3120, 'FM': 3120, 'GMSK': 3120, 'OQPSK': 3120}\n"
     ]
    }
   ],
   "source": [
    "classes = classes = [ 'OOK','4ASK','8ASK','BPSK', 'QPSK','8PSK','16PSK','32PSK','16APSK', '32APSK','64APSK','128APSK',\n",
    "        '16QAM', '32QAM','64QAM','128QAM','256QAM','AM-SSB-WC','AM-SSB-SC','AM-DSB-WC',\n",
    "        'AM-DSB-SC','FM', 'GMSK','OQPSK']\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def check_distribution(labels, classes):\n",
    "    distribution = {class_name: np.sum(labels[:, i]) for i, class_name in enumerate(classes)}\n",
    "    return distribution\n",
    "\n",
    "\n",
    "# Check the distribution in the training set\n",
    "train_distribution = check_distribution(Y_train, classes)\n",
    "print(\"Modulated signal distribution in the training set：\")\n",
    "print(train_distribution)\n",
    "\n",
    "# Check the distribution in the test set\n",
    "test_distribution = check_distribution(Y_test, classes)\n",
    "print(\"\\nModulated signal distribution in test set：\")\n",
    "print(test_distribution)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:37:52.647338800Z",
     "start_time": "2023-12-10T00:37:52.583167800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Convert Y_train, Y_valid and Y_test from one-hot encoding to category index\n",
    "Y_train_indices = np.argmax(Y_train, axis=1)\n",
    "Y_valid_indices = np.argmax(Y_valid, axis=1)\n",
    "Y_test_indices = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Convert data to PyTorch Tensor\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_indices, dtype=torch.long)\n",
    "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
    "Y_valid_tensor = torch.tensor(Y_valid_indices, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test_indices, dtype=torch.long)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, Y_valid_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:37:56.555243Z",
     "start_time": "2023-12-10T00:37:52.648337800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modulated signal distribution in the training set: Counter({16: 24960, 5: 24960, 13: 24960, 18: 24960, 14: 24960, 9: 24960, 11: 24960, 21: 24960, 12: 24960, 4: 24960, 6: 24960, 7: 24960, 10: 24960, 3: 24960, 15: 24960, 19: 24960, 8: 24960, 22: 24960, 2: 24960, 1: 24960, 0: 24960, 23: 24960, 17: 24960, 20: 24960})\n",
      "Modulation signal distribution in the test set: Counter({16: 3120, 21: 3120, 15: 3120, 12: 3120, 8: 3120, 4: 3120, 23: 3120, 0: 3120, 17: 3120, 10: 3120, 6: 3120, 5: 3120, 9: 3120, 1: 3120, 18: 3120, 14: 3120, 20: 3120, 3: 3120, 19: 3120, 22: 3120, 11: 3120, 7: 3120, 13: 3120, 2: 3120})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def check_distribution(loader):\n",
    "    counter = Counter()\n",
    "    for _, targets in loader:\n",
    "        counter.update(targets.tolist())\n",
    "    return counter\n",
    "\n",
    "# Check the distribution of training and test sets\n",
    "train_distribution = check_distribution(train_loader)\n",
    "test_distribution = check_distribution(test_loader)\n",
    "\n",
    "print(\"Modulated signal distribution in the training set:\", train_distribution)\n",
    "print(\"Modulation signal distribution in the test set:\", test_distribution)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:38:33.817809100Z",
     "start_time": "2023-12-10T00:37:56.564243400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Define residual stacking\n",
    "# class ResidualStack(nn.Module):\n",
    "#     def __init__(self, input_channels, output_channels, kernel_size, pool_size):\n",
    "#         super(ResidualStack, self).__init__()\n",
    "#         self.conv1x1 = nn.Conv2d(input_channels, output_channels, kernel_size=(1, 1), stride=1, padding='same')\n",
    "#         self.conv2 = nn.Conv2d(output_channels, 32, kernel_size=kernel_size, stride=1, padding='same')\n",
    "#         self.conv3 = nn.Conv2d(32, output_channels, kernel_size=kernel_size, stride=1, padding='same')\n",
    "#         self.conv4 = nn.Conv2d(output_channels, 32, kernel_size=kernel_size, stride=1, padding='same')\n",
    "#         self.conv5 = nn.Conv2d(32, output_channels, kernel_size=kernel_size, stride=1, padding='same')\n",
    "#         self.pool = nn.MaxPool2d(pool_size)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1x1(x)\n",
    "#\n",
    "#         # Residual Unit 1\n",
    "#         shortcut = x\n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = self.conv3(x)\n",
    "#         x += shortcut\n",
    "#         x = F.relu(x)\n",
    "#\n",
    "#         # Residual Unit 2\n",
    "#         shortcut = x\n",
    "#         x = F.relu(self.conv4(x))\n",
    "#         x = self.conv5(x)\n",
    "#         x += shortcut\n",
    "#         x = F.relu(x)\n",
    "#\n",
    "#         x = self.pool(x)\n",
    "#         return x\n",
    "#\n",
    "#\n",
    "# # Define the complete model\n",
    "# class ModulationClassificationModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(ModulationClassificationModel, self).__init__()\n",
    "#         self.res_stack0 = ResidualStack(1, 32, kernel_size=(3, 2), pool_size=(2, 2))\n",
    "#         self.res_stack1 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "#         self.res_stack2 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "#         self.res_stack3 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "#         self.res_stack4 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "#         self.res_stack5 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc1 = nn.Linear(32 * 16 * 1, 128)  # Adjust the input size based on the output of the last ResidualStack\n",
    "#         self.alpha_dropout = nn.AlphaDropout(0.2)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)  # Add a channel dimension\n",
    "#         x = self.res_stack0(x)\n",
    "#         x = self.res_stack1(x)\n",
    "#         x = self.res_stack2(x)\n",
    "#         x = self.res_stack3(x)\n",
    "#         x = self.res_stack4(x)\n",
    "#         x = self.res_stack5(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = F.selu(self.fc1(x))\n",
    "#         x = self.alpha_dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义残差单元\n",
    "class ResidualUnit(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=kernel_size, stride=1, padding='same')\n",
    "        self.conv2 = nn.Conv2d(32, output_channels, kernel_size=kernel_size, stride=1, padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x += shortcut\n",
    "        return F.relu(x)\n",
    "\n",
    "# 定义残差堆叠\n",
    "class ResidualStack(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, pool_size):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.conv1x1 = nn.Conv2d(input_channels, output_channels, kernel_size=(1, 1), stride=1, padding='same')\n",
    "        self.res_unit1 = ResidualUnit(output_channels, output_channels, kernel_size)\n",
    "        self.res_unit2 = ResidualUnit(output_channels, output_channels, kernel_size)\n",
    "        self.pool = nn.MaxPool2d(pool_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.res_unit1(x)\n",
    "        x = self.res_unit2(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "# 定义完整模型\n",
    "class ModulationClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModulationClassificationModel, self).__init__()\n",
    "        self.reshape = nn.Unflatten(1, (1, 1024, 2))\n",
    "        self.res_stack0 = ResidualStack(1, 32, kernel_size=(3, 2), pool_size=(2, 2))\n",
    "        self.res_stack1 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "        self.res_stack2 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "        self.res_stack3 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "        self.res_stack4 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "        self.res_stack5 = ResidualStack(32, 32, kernel_size=(3, 1), pool_size=(2, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32 * 16 * 1, 128)  # 根据池化后的特征图尺寸调整\n",
    "        self.alpha_dropout = nn.AlphaDropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.res_stack0 = ResidualStack(1, 32, kernel_size=(3, 2), pool_size=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # 增加一个维度以匹配卷积层的输入要求\n",
    "        x = self.res_stack0(x)\n",
    "        x = self.res_stack1(x)\n",
    "        x = self.res_stack2(x)\n",
    "        x = self.res_stack3(x)\n",
    "        x = self.res_stack4(x)\n",
    "        x = self.res_stack5(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.selu(self.fc1(x))\n",
    "        x = self.alpha_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ModulationClassificationModel(num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:38:33.952259800Z",
     "start_time": "2023-12-10T00:38:33.852145100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100: 100%|██████████| 2340/2340 [02:47<00:00, 13.96it/s, accuracy=28.4, loss=0.00805]\n",
      "Validating Epoch 1/100: 100%|██████████| 293/293 [00:12<00:00, 22.73it/s, accuracy=38.1, loss=0.00698]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Valid Loss: 1.7836, Valid Accuracy: 38.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/100: 100%|██████████| 2340/2340 [02:50<00:00, 13.75it/s, accuracy=38.9, loss=0.00681]\n",
      "Validating Epoch 2/100: 100%|██████████| 293/293 [00:09<00:00, 29.95it/s, accuracy=40.7, loss=0.00679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Valid Loss: 1.7344, Valid Accuracy: 40.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/100: 100%|██████████| 2340/2340 [02:50<00:00, 13.69it/s, accuracy=42.4, loss=0.00642]\n",
      "Validating Epoch 3/100: 100%|██████████| 293/293 [00:08<00:00, 32.61it/s, accuracy=41.2, loss=0.00669]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Valid Loss: 1.7109, Valid Accuracy: 41.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/100: 100%|██████████| 2340/2340 [02:51<00:00, 13.61it/s, accuracy=44.9, loss=0.00619]\n",
      "Validating Epoch 4/100: 100%|██████████| 293/293 [00:08<00:00, 33.46it/s, accuracy=46.8, loss=0.00615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Valid Loss: 1.5710, Valid Accuracy: 46.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/100: 100%|██████████| 2340/2340 [02:51<00:00, 13.68it/s, accuracy=47.3, loss=0.00596]\n",
      "Validating Epoch 5/100: 100%|██████████| 293/293 [00:08<00:00, 32.74it/s, accuracy=47.6, loss=0.00605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Valid Loss: 1.5456, Valid Accuracy: 47.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/100: 100%|██████████| 2340/2340 [02:48<00:00, 13.87it/s, accuracy=48.9, loss=0.00579]\n",
      "Validating Epoch 6/100: 100%|██████████| 293/293 [00:08<00:00, 34.43it/s, accuracy=47.8, loss=0.00609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Valid Loss: 1.5554, Valid Accuracy: 47.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/100: 100%|██████████| 2340/2340 [02:47<00:00, 13.93it/s, accuracy=50.4, loss=0.00563]\n",
      "Validating Epoch 7/100: 100%|██████████| 293/293 [00:08<00:00, 34.04it/s, accuracy=50.3, loss=0.00573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Valid Loss: 1.4637, Valid Accuracy: 50.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/100: 100%|██████████| 2340/2340 [02:47<00:00, 13.96it/s, accuracy=51.7, loss=0.00552]\n",
      "Validating Epoch 8/100: 100%|██████████| 293/293 [00:08<00:00, 34.41it/s, accuracy=53, loss=0.00552]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Valid Loss: 1.4112, Valid Accuracy: 53.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/100: 100%|██████████| 2340/2340 [02:48<00:00, 13.87it/s, accuracy=53.1, loss=0.00542]\n",
      "Validating Epoch 9/100: 100%|██████████| 293/293 [00:08<00:00, 34.42it/s, accuracy=53.3, loss=0.00556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Valid Loss: 1.4208, Valid Accuracy: 53.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/100: 100%|██████████| 2340/2340 [02:48<00:00, 13.87it/s, accuracy=54.1, loss=0.00532]\n",
      "Validating Epoch 10/100: 100%|██████████| 293/293 [00:08<00:00, 34.13it/s, accuracy=54.8, loss=0.00553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Valid Loss: 1.4121, Valid Accuracy: 54.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11/100: 100%|██████████| 2340/2340 [02:50<00:00, 13.73it/s, accuracy=54.8, loss=0.00527]\n",
      "Validating Epoch 11/100: 100%|██████████| 293/293 [00:08<00:00, 33.90it/s, accuracy=55.6, loss=0.00531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Valid Loss: 1.3576, Valid Accuracy: 55.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12/100: 100%|██████████| 2340/2340 [02:49<00:00, 13.84it/s, accuracy=55.1, loss=0.00523]\n",
      "Validating Epoch 12/100: 100%|██████████| 293/293 [00:08<00:00, 33.77it/s, accuracy=55.2, loss=0.00535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Valid Loss: 1.3666, Valid Accuracy: 55.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13/100: 100%|██████████| 2340/2340 [02:51<00:00, 13.65it/s, accuracy=55.6, loss=0.00518]\n",
      "Validating Epoch 13/100: 100%|██████████| 293/293 [00:08<00:00, 33.43it/s, accuracy=55.5, loss=0.00536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Valid Loss: 1.3699, Valid Accuracy: 55.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14/100:  51%|█████     | 1194/2340 [01:27<01:23, 13.66it/s, accuracy=55.9, loss=0.00515]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 59\u001B[0m\n\u001B[0;32m     56\u001B[0m             best_valid_loss \u001B[38;5;241m=\u001B[39m avg_valid_loss\n\u001B[0;32m     57\u001B[0m             torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbest_model.pth\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 59\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalid_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 24\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, criterion, optimizer, train_loader, valid_loader, device, num_epochs)\u001B[0m\n\u001B[0;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     23\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m---> 24\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m train_correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m preds\u001B[38;5;241m.\u001B[39meq(target)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     26\u001B[0m total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, valid_loader, device, num_epochs=100):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total = 0\n",
    "\n",
    "        train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch + 1}/{num_epochs}')\n",
    "        for data, target in train_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            train_correct += preds.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "            train_bar.set_postfix(loss=train_loss/total, accuracy=100.*train_correct/total)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        valid_bar = tqdm(valid_loader, desc=f'Validating Epoch {epoch + 1}/{num_epochs}')\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_bar:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                valid_loss += loss.item()\n",
    "                _, preds = torch.max(output, 1)\n",
    "                valid_correct += preds.eq(target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "                valid_bar.set_postfix(loss=valid_loss/valid_total, accuracy=100.*valid_correct/valid_total)\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_accuracy = 100. * valid_correct / valid_total\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "train_model(model, criterion, optimizer, train_loader, valid_loader, device, num_epochs=100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T01:18:44.367729200Z",
     "start_time": "2023-12-10T00:38:33.960260300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            _, preds = torch.max(output, 1)\n",
    "            correct += preds.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# 加载最佳模型并进行测试\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_model(model, test_loader, device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-10T01:18:44.356785500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "modulation",
   "language": "python",
   "display_name": "modulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
